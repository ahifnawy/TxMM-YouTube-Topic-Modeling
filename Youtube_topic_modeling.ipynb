{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBaa3fWF8eok",
        "outputId": "859adce6-206b-40ac-a305-54362e63d2a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/db/1e/af4e9cded5093a92e60d4ae7149a02c7427661b2db66c8ea4d34b17864a2/sklearn-0.0.post1.tar.gz#sha256=76b9ed1623775168657b86b5fe966d45752e5c87f528de6240c38923b94147c5 (from https://pypi.org/simple/sklearn/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.2.0)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=b9fa4939c13eda1d0b1fc0ab5aff33e71b0328afaa132c8c0174df362e29e753\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install nltk\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://transfer.sh/St0XVR/Youtube01-Psy.csv\n",
        "!wget https://transfer.sh/u4GXsd/Youtube02-KatyPerry.csv\n",
        "!wget https://transfer.sh/E58o9N/Youtube03-LMFAO.csv\n",
        "!wget https://transfer.sh/tGSeyT/Youtube04-Eminem.csv\n",
        "!wget https://transfer.sh/2SuGzp/Youtube05-Shakira.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKXxM1TtZXmv",
        "outputId": "801e8bf0-84cc-490d-8f2d-59465de8dc7f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-30 13:04:59--  https://transfer.sh/St0XVR/Youtube01-Psy.csv\n",
            "Resolving transfer.sh (transfer.sh)... 144.76.136.153, 2a01:4f8:200:1097::2\n",
            "Connecting to transfer.sh (transfer.sh)|144.76.136.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 57438 (56K) [text/csv]\n",
            "Saving to: ‘Youtube01-Psy.csv’\n",
            "\n",
            "Youtube01-Psy.csv   100%[===================>]  56.09K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-12-30 13:05:00 (408 KB/s) - ‘Youtube01-Psy.csv’ saved [57438/57438]\n",
            "\n",
            "--2022-12-30 13:05:00--  https://transfer.sh/u4GXsd/Youtube02-KatyPerry.csv\n",
            "Resolving transfer.sh (transfer.sh)... 144.76.136.153, 2a01:4f8:200:1097::2\n",
            "Connecting to transfer.sh (transfer.sh)|144.76.136.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64279 (63K) [text/csv]\n",
            "Saving to: ‘Youtube02-KatyPerry.csv’\n",
            "\n",
            "Youtube02-KatyPerry 100%[===================>]  62.77K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-12-30 13:05:01 (450 KB/s) - ‘Youtube02-KatyPerry.csv’ saved [64279/64279]\n",
            "\n",
            "--2022-12-30 13:05:02--  https://transfer.sh/E58o9N/Youtube03-LMFAO.csv\n",
            "Resolving transfer.sh (transfer.sh)... 144.76.136.153, 2a01:4f8:200:1097::2\n",
            "Connecting to transfer.sh (transfer.sh)|144.76.136.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64419 (63K) [text/csv]\n",
            "Saving to: ‘Youtube03-LMFAO.csv’\n",
            "\n",
            "Youtube03-LMFAO.csv 100%[===================>]  62.91K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-12-30 13:05:03 (638 KB/s) - ‘Youtube03-LMFAO.csv’ saved [64419/64419]\n",
            "\n",
            "--2022-12-30 13:05:03--  https://transfer.sh/tGSeyT/Youtube04-Eminem.csv\n",
            "Resolving transfer.sh (transfer.sh)... 144.76.136.153, 2a01:4f8:200:1097::2\n",
            "Connecting to transfer.sh (transfer.sh)|144.76.136.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 82896 (81K) [text/csv]\n",
            "Saving to: ‘Youtube04-Eminem.csv’\n",
            "\n",
            "Youtube04-Eminem.cs 100%[===================>]  80.95K   475KB/s    in 0.2s    \n",
            "\n",
            "2022-12-30 13:05:04 (475 KB/s) - ‘Youtube04-Eminem.csv’ saved [82896/82896]\n",
            "\n",
            "--2022-12-30 13:05:04--  https://transfer.sh/2SuGzp/Youtube05-Shakira.csv\n",
            "Resolving transfer.sh (transfer.sh)... 144.76.136.153, 2a01:4f8:200:1097::2\n",
            "Connecting to transfer.sh (transfer.sh)|144.76.136.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72706 (71K) [text/csv]\n",
            "Saving to: ‘Youtube05-Shakira.csv’\n",
            "\n",
            "Youtube05-Shakira.c 100%[===================>]  71.00K   453KB/s    in 0.2s    \n",
            "\n",
            "2022-12-30 13:05:06 (453 KB/s) - ‘Youtube05-Shakira.csv’ saved [72706/72706]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "youtube_1=pd.read_csv(\"/content/Youtube01-Psy.csv\")\n",
        "youtube_2=pd.read_csv(\"/content/Youtube02-KatyPerry.csv\")\n",
        "youtube_3=pd.read_csv(\"/content/Youtube03-LMFAO.csv\")\n",
        "youtube_4=pd.read_csv(\"/content/Youtube04-Eminem.csv\")\n",
        "youtube_5=pd.read_csv(\"/content/Youtube05-Shakira.csv\")\n",
        "\n",
        "comments=[youtube_1, youtube_2, youtube_3, youtube_4, youtube_5]\n"
      ],
      "metadata": {
        "id": "T3FOKwYgcGY1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "def clean_comment(text):\n",
        "  #text to lowercase\n",
        "  text=text.lower()\n",
        "  #using a regex to replace links with LINK\n",
        "  text=re.sub('http\\S+','LINK',text) \n",
        "  #using a regex to replace punctuation with whitespace\n",
        "  text=re.sub(r'[^\\w\\s]', ' ',text)\n",
        "  #removing extra whitespaces\n",
        "  text=re.sub(r'\\s+',' ',text)\n",
        "\n",
        "  #removing leading and trailing whitespaces\n",
        "  text=text.strip()\n",
        "  return text\n",
        "\n",
        "print(clean_comment(\"http://www.billboard.com/articles/columns/pop-shop/6174122/fan-army-face-off-round-3 Vote for SONES please....we're against vips....please help us.. & gt ; . & lt;﻿'\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxooECTvIMhk",
        "outputId": "20ec502d-1ae6-42fa-9d52-80150b52ab1e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LINK vote for sones please we re against vips please help us gt lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "vid_index=1\n",
        "\n",
        "for df in comments: \n",
        "  df=df[['CONTENT', 'CLASS']].copy()\n",
        "  df.CONTENT=df.CONTENT.map(clean_comment)\n",
        "  count_vect = CountVectorizer(max_df=0.8, min_df=5, stop_words='english')\n",
        "  doc_term_matrix = count_vect.fit_transform(df['CONTENT'].values.astype('U'))\n",
        "  LDA = LatentDirichletAllocation(n_components=4)\n",
        "  LDA.fit(doc_term_matrix)\n",
        "  for i,topic in enumerate(LDA.components_):\n",
        "    index=i+1\n",
        "    print(f'Top 10 words for topic #{index} for video #{vid_index}:')\n",
        "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
        "    print('\\n')\n",
        "  vid_index+=1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR3kePvmHyAy",
        "outputId": "c5f77141-5916-482d-fb3d-0c7aeb788e22"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 words for topic #1 for video #1:\n",
            "['comment', 'plz', 'thanks', 'hey', 'videos', 'guys', 'like', 'channel', 'subscribe', 'check']\n",
            "\n",
            "\n",
            "Top 10 words for topic #2 for video #1:\n",
            "['new', 'video', 'im', 'gangnam', 'style', 'don', 'music', 'just', 'psy', 'song']\n",
            "\n",
            "\n",
            "Top 10 words for topic #3 for video #1:\n",
            "['wow', 'day', 'dont', 'song', 'earth', 'comments', 'know', 'free', 'money', 'like']\n",
            "\n",
            "\n",
            "Top 10 words for topic #4 for video #1:\n",
            "['world', 'funny', 'just', 'lol', 'people', 'youtube', 'video', 'billion', 'views', 'link']\n",
            "\n",
            "\n",
            "Top 10 words for topic #1 for video #2:\n",
            "['birthday', 'shit', 'songs', 'nice', 'lt', 'roar', 'song', 'love', 'perry', 'katy']\n",
            "\n",
            "\n",
            "Top 10 words for topic #2 for video #2:\n",
            "['just', 'awesome', 'music', 'youtube', 'tiger', 'good', 'best', 'guys', 'video', 'song']\n",
            "\n",
            "\n",
            "Top 10 words for topic #3 for video #2:\n",
            "['watch', 'thanks', 'new', 'video', 'really', 'gt', 'don', 'like', 'channel', 'check']\n",
            "\n",
            "\n",
            "Top 10 words for topic #4 for video #2:\n",
            "['thank', 'make', 'like', 'free', 'watch', 'help', 'subscribe', 'follow', 'videos', 'link']\n",
            "\n",
            "\n",
            "Top 10 words for topic #1 for video #3:\n",
            "['thank', 'channel', 'im', 'old', 'look', 'thumbs', 'playlist', 'video', 'youtube', 'check']\n",
            "\n",
            "\n",
            "Top 10 words for topic #2 for video #3:\n",
            "['dance', 'rock', 'watching', 'comment', '2015', 'good', 'party', 'love', 'like', 'song']\n",
            "\n",
            "\n",
            "Top 10 words for topic #3 for video #3:\n",
            "['thank', 'got', 'like', 'people', 'share', 'best', 'nice', 'href', 'link', 'br']\n",
            "\n",
            "\n",
            "Top 10 words for topic #4 for video #3:\n",
            "['fucking', 'subscribers', 'videos', 'class', 'views', 'music', 'cool', 'channel', '39', 'subscribe']\n",
            "\n",
            "\n",
            "Top 10 words for topic #1 for video #4:\n",
            "['way', 'hey', 'just', 'im', 'music', 'love', 'video', 'channel', 'youtube', 'check']\n",
            "\n",
            "\n",
            "Top 10 words for topic #2 for video #4:\n",
            "['visit', 'making', 'does', 'guys', 'working', 'money', 'called', 'moneygq', 'check', 'com']\n",
            "\n",
            "\n",
            "Top 10 words for topic #3 for video #4:\n",
            "['people', 'know', 'thank', 'love', 'just', 'quot', 'subscribe', 'song', 'like', '39']\n",
            "\n",
            "\n",
            "Top 10 words for topic #4 for video #4:\n",
            "['song', 'megan', 'fox', 'rap', 'new', 'rihanna', 'check', 'video', 'br', 'eminem']\n",
            "\n",
            "\n",
            "Top 10 words for topic #1 for video #5:\n",
            "['nice', 'beautiful', 'thanks', 'shakira', 'good', 'channel', 'like', 'subscribe', 'love', 'song']\n",
            "\n",
            "\n",
            "Top 10 words for topic #2 for video #5:\n",
            "['make', 'video', '39', 'money', 'com', 'quot', 'waka', 'shakira', 'check', 'br']\n",
            "\n",
            "\n",
            "Top 10 words for topic #3 for video #5:\n",
            "['hey', 'thank', 'really', 'comment', 'covers', 'just', 'music', '39', 'new', 'check']\n",
            "\n",
            "\n",
            "Top 10 words for topic #4 for video #5:\n",
            "['twitter', '4netjobs', 'work', 'song', 'paid', 'make', 'best', 'com', 'world', 'facebook']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#code reference https://stackabuse.com/python-for-nlp-topic-modeling/\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "vid_index=1\n",
        "for df in comments: \n",
        "  df=df[['CONTENT', 'CLASS']].copy()\n",
        "  df.CONTENT=df.CONTENT.map(clean_comment)\n",
        "  grouped=df.groupby(['CLASS'])\n",
        "  legit=grouped.get_group(0)\n",
        "  spam=grouped.get_group(1)\n",
        "  \n",
        "  count_vect = CountVectorizer(max_df=0.8, min_df=5, stop_words='english')\n",
        "  doc_term_matrix_legit = count_vect.fit_transform(legit['CONTENT'].values.astype('U'))\n",
        "\n",
        "  LDA_legit = LatentDirichletAllocation(n_components=4)\n",
        "  LDA_legit.fit(doc_term_matrix_legit)\n",
        "  \n",
        "  for i,topic in enumerate(LDA_legit.components_):\n",
        "    index=i+1\n",
        "    print(f'Top 10 words for Legit topic #{index} for video #{vid_index}:')\n",
        "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
        "    print('\\n')\n",
        "\n",
        "  doc_term_matrix_spam = count_vect.fit_transform(spam['CONTENT'].values.astype('U'))\n",
        "\n",
        "  LDA_spam = LatentDirichletAllocation(n_components=4)\n",
        "  LDA_spam.fit(doc_term_matrix_spam)\n",
        "  \n",
        "  for i,topic in enumerate(LDA_spam.components_):\n",
        "    index=i+1\n",
        "    print(f'Top 10 words for Spam topic #{index} for video #{vid_index}::')\n",
        "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
        "    print('\\n')\n",
        "\n",
        "  vid_index+=1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV8k55zCWmMI",
        "outputId": "64c8d34d-92d7-448c-b495-a68646b9f310"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 words for Legit topic #1 for video #1:\n",
            "['old', 'think', 'wow', 'comment', '강남스타일', 'shit', 'lol', 'style', 'gangnam', 'psy']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #2 for video #1:\n",
            "['wow', 'funny', 'thing', 'earth', 'world', 'viewed', 'youtube', 'billion', 'video', 'views']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #3 for video #1:\n",
            "['guy', 'think', 'old', 'funny', 'does', 'million', 'billion', 'people', 'views', 'song']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #4 for video #1:\n",
            "['like', 'comments', 'watching', 'people', 'years', 'came', 'music', 'check', 'views', 'just']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #1 for video #1::\n",
            "['new', 'think', 'hi', 'minecraft', 'thanks', 'hey', 'videos', 'guys', 'channel', 'check']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #2 for video #1::\n",
            "['new', 'watch', 'song', 'sub', 'video', 'subscribe', 'check', 'plz', 'music', 'channel']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #3 for video #1::\n",
            "['want', 'com', 'free', 'new', 'comment', 'money', 'subscribers', 'll', 'like', 'subscribe']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #4 for video #1::\n",
            "['song', 'time', 'thanks', 'help', 'money', 'free', 'just', 'don', 'follow', 'link']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #1 for video #2:\n",
            "['shit', 'like', 'beautiful', 'great', 'roar', 'hear', 'video', 'good', 'love', 'song']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #2 for video #2:\n",
            "['know', 'shit', 'million', 'really', 'views', 'tiger', 'don', 'song', 'just', 'like']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #3 for video #2:\n",
            "['link', 'awesome', 'old', 'official', 'songs', 'roar', 'lt', 'love', 'perry', 'katy']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #4 for video #2:\n",
            "['beautiful', 'know', 'million', 'tiger', 'people', 'world', 'jungle', 'music', 'best', 'video']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #1 for video #2::\n",
            "['guys', 'amp', 'money', 'new', 'help', 'lt', 'gt', 'like', 'free', 'link']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #2 for video #2::\n",
            "['videos', 'thumbs', 'watch', 'like', 'love', 'perry', 'video', 'katy', 'channel', 'subscribe']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #3 for video #2::\n",
            "['listen', 'video', 'just', 'check', 'music', 'youtube', 'videos', 'watch', 'song', 'follow']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #4 for video #2::\n",
            "['com', 'thank', 'hey', 'katy', 'look', 'channel', 'thanks', 'guys', 'vote', 'check']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #1 for video #3:\n",
            "['years', 'shuffling', '2015', 'old', 'awesome', 'video', 'best', 'good', 'like', 'song']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #2 for video #3:\n",
            "['shuffling', 'songs', 'lmfao', 'years', '2015', 'xd', 'shuffle', '39', 'rock', 'party']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #3 for video #3:\n",
            "['nice', 'video', 'party', 'rock', 'like', 'shuffling', '39', 'views', 'music', 'br']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #4 for video #3:\n",
            "['songs', 'just', 'time', 'makes', 'nice', 'dance', 'lmfao', 'cool', 'song', 'love']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #1 for video #3::\n",
            "['song', 'help', 'just', 'reason', '2015', '39', 'subscribers', 'watching', 'comment', 'like']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #2 for video #3::\n",
            "['guys', 'want', 'channel', 'im', 'thumbs', 'music', 'love', 'playlist', 'youtube', 'check']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #3 for video #3::\n",
            "['just', 'videos', 'quot', 'class', 'href', 'link', '39', 'channel', 'subscribe', 'br']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #4 for video #3::\n",
            "['just', 'quot', 'br', 'song', 'look', 'share', 'views', 'youtube', 'check', 'video']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #1 for video #4:\n",
            "['good', 'fuck', 'years', '2015', 'lie', 'way', 'eminem', 'best', 'song', 'love']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #2 for video #4:\n",
            "['billion', 'fuck', 'guy', 'awesome', 'video', 'views', 'charlie', 'lost', 'megan', 'fox']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #3 for video #4:\n",
            "['2015', 'songs', 'better', 'know', 'billion', 'rap', 'song', 'like', 'eminem', '39']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #4 for video #4:\n",
            "['songs', 'guy', 'video', 'eminem', 'good', 'just', 'like', 'music', 'rihanna', 'br']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #1 for video #4::\n",
            "['know', 'dream', 'thank', 'love', 'just', 'check', 'like', '39', 'quot', 'subscribe']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #2 for video #4::\n",
            "['year', 'guys', 'rapper', 'check', 'hey', 'like', 'channel', 'im', 'just', '39']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #3 for video #4::\n",
            "['playlist', 'watch', 'eminem', 'called', 'song', 'new', 'channel', 'youtube', 'video', 'check']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #4 for video #4::\n",
            "['make', 'today', 'visit', 'does', '000', 'working', 'money', 'moneygq', 'check', 'com']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #1 for video #5:\n",
            "['time', 'lt', 'years', 'shakira', 'good', 'nice', 'br', 'waka', 'song', 'love']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #2 for video #5:\n",
            "['cup', 'wow', 'shakira', 'love', 'song', 'br', '2015', 'watching', 'years', 'like']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #3 for video #5:\n",
            "['world', 'cup', 'br', 'good', 'song', 'time', 'lt', 'shakira', 'wow', '39']\n",
            "\n",
            "\n",
            "Top 10 words for Legit topic #4 for video #5:\n",
            "['like', 'nice', 'time', 'love', 'cup', 'world', 'beautiful', 'song', 'best', 'shakira']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #1 for video #5::\n",
            "['video', 'youtube', 'online', 'visit', 'website', 'make', 'money', 'com', 'new', 'check']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #2 for video #5::\n",
            "['check', 'really', 'money', 'thank', 'covers', 'chance', 'comment', 'just', '39', 'music']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #3 for video #5::\n",
            "['href', 'income', 'make', 'twitter', '4netjobs', 'work', 'facebook', 'paid', 'com', 'link']\n",
            "\n",
            "\n",
            "Top 10 words for Spam topic #4 for video #5::\n",
            "['love', 'just', 'free', 'song', 'like', 'channel', 'quot', '39', 'subscribe', 'br']\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}